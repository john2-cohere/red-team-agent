from __future__ import annotations

import traceback
import json
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO
from types import SimpleNamespace
from typing import Dict, Optional, Tuple, Callable, List, ClassVar
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass

from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import BaseModel
from logging import getLogger

from pentest_bot.agent.logger import setup_agent_logger, AgentLogLevels
from src.llm_providers import llm_providers, LLMProviders
from src.utils import dump_llm_messages_pretty, retry_sync, EarlyShutdown
from eval.challenges import ExploitChallenge

from .data import JUICESHOP_EXPLOIT_CHALLENGES
from .prompts import (
    SQL_GUIDE_O3,
    NORMAL_PROMPT,
)

logger = getLogger(AgentLogLevels.AGENT)
full_logger = getLogger(AgentLogLevels.FULL_REQUESTS)

# ---------------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------------


class ExploitStepResult(BaseModel):
    """Result of a single exploit step returned by the LLM."""

    reflection: str
    script: str
    answer: Optional[Dict] = None

    model_schema: ClassVar[Dict] = {
        "type": "json_object",
        "required": ["reflection", "script"],
        "properties": {
            "reflection": {
                "type": "string",
                "description": "Natural language reasoning about the next step",
            },
            "script": {"type": "string", "description": "Python code to execute"},
            "answer": {
                "type": "object",
                "description": "Optional answer if exploit is found",
            },
        },
    }

    def is_complete(self) -> bool:
        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection


# ---------------------------------------------------------------------------
# Prompt construction
# ---------------------------------------------------------------------------


def build_prompt(
    *,
    problem_stmt: str,
    prev_turn: Optional[SimpleNamespace],
    agent_prompt: Optional[str],
) -> str:
    """Return the wrapper prompt for the next exploit step.

    Parameters
    ----------
    problem_stmt:
        The challenge description (inserted once at the beginning of every run).
    prev_turn:
        Namespace containing *reflection*, *script* and *output* from the
        previous agent action.  Omitted on the first step.
    agent_prompt:
        The strategy / instructions unique to this agent variant.  If *None*,
        defaults to :data:`SQLI_PLAYBOOK_TEMPLATE`.
    """

    template_parts: List[str] = []

    if problem_stmt:
        template_parts.append(
            f"""
<problem_stmt>
Here is the CTF problem statement:
{problem_stmt}
</problem_stmt>
"""
        )

    if prev_turn:
        template_parts.append(
            f"""
<prev_reflection>
Here is the reflection from the previous action:
{prev_turn.reflection}
</prev_reflection>

<prev_script>
Here is the generated script:
{prev_turn.script}
</prev_script>

<prev_output>
Here is the output from the previous action:
{prev_turn.output}
</prev_output>
"""
        )

    # Main instructions for this agent
    template_parts.append(agent_prompt)
    template_parts.append("""
Return your answer in JSON using the ExploitStepResult schema:
{
        "type": "json_object",
        "required": ["reflection", "script"],
        "properties": {
            "reflection": {
                "type": "string",
                "description": "Natural language reasoning about the next step",
            },
            "script": {"type": "string", "description": "Python code to execute"},
            "answer": {
                "type": "object",
                "description": "Optional answer if exploit is found",
            },
        },
}   
""")

    return "".join(template_parts)


# ---------------------------------------------------------------------------
# Interpreter wrapper
# ---------------------------------------------------------------------------


class PythonInterpreter:
    """Executes untrusted Python code and captures combined stdout/stderr."""

    def __init__(self, shared_globals: Optional[Dict] = None) -> None:
        self._globals: Dict = shared_globals or {}

    def run(self, code: str) -> str:
        stdout_buf, stderr_buf = StringIO(), StringIO()
        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except Exception:  # nosec B110 – we *want* to capture the traceback
            traceback.print_exc(file=stderr_buf)
        return stdout_buf.getvalue() + ("\n" + stderr_buf.getvalue() if stderr_buf.getvalue() else "")


# ---------------------------------------------------------------------------
# Core agent logic
# ---------------------------------------------------------------------------


def exploit_step(
    *,
    llm: BaseChatModel,
    prompt: str,
) -> ExploitStepResult:
    """Invoke the LLM for one exploit iteration."""

    messages = [{"role": "user", "content": prompt}]
    full_logger.info("[PROMPT EXPLOIT STEP]:\n%s", dump_llm_messages_pretty(messages))

    res = llm.invoke(messages, response_format=ExploitStepResult.model_schema)
    result = ExploitStepResult(**json.loads(res.content))

    logger.info("[EXPLOIT STEP] Reflection:\n%s", result.reflection.strip())
    logger.info("[EXPLOIT STEP] Generated script:\n%s", result.script.rstrip())
    if result.answer:
        logger.info("[EXPLOIT STEP] Found answer: %s", result.answer)

    return result


class PentestSession:
    """Drives the LLM ↔ interpreter feedback loop."""

    def __init__(
        self,
        *,
        problem_stmt: str,
        model: LLMProviders,
        max_steps: int,
        success_condition: str,
        agent_prompt: Optional[str],
        eval_fn: Optional[Callable] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self.success_condition = success_condition
        self._model = model
        self._interp = PythonInterpreter()
        self._success = False
        self._steps = 0
        self.agent_prompt = agent_prompt

        # Success evaluator
        if eval_fn:
            self._is_success = eval_fn
        elif success_condition:
            self._is_success = self._create_llm_evaluator()
        else:
            self._is_success = lambda _x: True

    # ---------------------------------------------------------------------
    # Evaluation helper
    # ---------------------------------------------------------------------

    def _create_llm_evaluator(self) -> Callable[[str], bool]:
        def _llm_eval(answer: str) -> bool:
            if not answer:
                return False
            prompt = (
                "Success Condition: "
                + self.success_condition
                + "\nAnswer to Evaluate: "
                + json.dumps(answer)
                + "\nRespond with only TRUE or FALSE."
            )
            res = self._model.get_client("default").invoke([{"role": "user", "content": prompt}])
            return "TRUE" in res.content.strip().upper()

        return _llm_eval

    # ---------------------------------------------------------------------
    # Main loop
    # ---------------------------------------------------------------------

    def run(self) -> None:
        prev_turn: Optional[SimpleNamespace] = None
        for step in range(1, self.max_steps + 1):
            logger.info("========== STEP %d ==========", step)

            prompt = build_prompt(
                problem_stmt=self.problem_stmt,
                prev_turn=prev_turn,
                agent_prompt=self.agent_prompt,
            )
            logger.info("[EXPLOIT PROMPT]:\n%s", prompt)

            result = exploit_step(
                llm=self._model.get_client("cohere"),
                prompt=prompt,
            )

            # Check success early if the model returned an answer object
            if result.answer and self._is_success(result.answer):
                logger.info("Agent successfully completed task with answer.")
                self._success = True
                return

            # Otherwise execute the script and continue the loop
            execution_output = self._interp.run(result.script)
            full_logger.info("Full execution output:\n%s", execution_output)
            logger.info("Truncated execution output:\n%s", execution_output[:8000])

            prev_turn = SimpleNamespace(
                reflection=result.reflection,
                script=result.script,
                output=execution_output,
            )
            self._steps += 1

        logger.warning("💀 Maximum steps (%d) reached without exploit.", self.max_steps)

    def result(self) -> Tuple[bool, int, int]:
        return self._success, self._steps, self.max_steps


# ---------------------------------------------------------------------------
# Runner helpers
# ---------------------------------------------------------------------------


def start_agent(
    *,
    challenge: ExploitChallenge,
    agent_prompt: Optional[str],
    model: LLMProviders,
    max_steps: int,
    vuln_app_url: str,
) -> Tuple[bool, int, int]:
    """Run a single challenge + prompt combination."""

    try:
        problem_stmt = challenge.prompt.format(url=vuln_app_url)
        setup_agent_logger(challenge.name)

        session = PentestSession(
            problem_stmt=problem_stmt,
            model=model,
            max_steps=max_steps,
            success_condition=challenge.success_condition,
            agent_prompt=agent_prompt,
        )
        session.run()
        return session.result()
    except Exception:
        logger.exception("Uncaught error in %s", challenge.name)
        return False, 0, max_steps


# ---------------------------------------------------------------------------
# Batch CLI for experimentation
# ---------------------------------------------------------------------------


def _main() -> None:
    """Execute all Juice Shop challenges against multiple agent prompts."""

    AGENT_PROMPTS: List[Optional[str]] = [
        # SQL_GUIDE_O3,
        NORMAL_PROMPT,
    ]
    AGENT_PROMPTS = AGENT_PROMPTS * 10

    MAX_STEPS = 12
    VULN_APP_URL = "http://localhost:3000"

    jobs: List[Tuple[ExploitChallenge, Optional[str]]] = [
        (challenge, prompt) for challenge in JUICESHOP_EXPLOIT_CHALLENGES for prompt in AGENT_PROMPTS
    ]

    @dataclass
    class Result:
        key: str
        success: bool
        steps: int
        max_steps: int

    results: Dict[str, Result] = {}
    model = llm_providers

    def _run_job(chal: ExploitChallenge, prmpt: Optional[str]) -> None:
        key = f"{chal.name}|{hash(prmpt) if prmpt else 'default'}"
        success, steps, m_steps = start_agent(
            challenge=chal,
            agent_prompt=prmpt,
            model=model,
            max_steps=MAX_STEPS,
            vuln_app_url=VULN_APP_URL,
        )
        results[key] = Result(key, success, steps, m_steps)

    with ThreadPoolExecutor() as pool:
        for chal, prmpt in jobs:
            pool.submit(_run_job, chal, prmpt)

    print("\nAgent Results:")
    print("-" * 50)
    for res in results.values():
        status = "✅ Success" if res.success else "❌ Failed"
        print(f"{res.key}: {status} (Steps: {res.steps}/{res.max_steps})")


if __name__ == "__main__":
    _main()
