# pentest_multi_turn_agent.py
"""A self‑contained multi‑turn pentesting agent runner.

This expands the single‑turn `PentestBot` prototype into a loop that
iteratively generates Python exploit scripts, executes them, feeds the
results back to the LLM, and stops when an exploit is confirmed or a
configured step‑limit is reached.

Dependencies
------------
* johnllm‑provided `LMP` and `LLMModel` abstractions
* pydantic >= 2.0

The code purposefully keeps the runtime sandbox simple (built on `exec`).
For production use you **must** swap in a hardened sandbox (e.g. "python‑seccomp",
`subprocess` inside a Docker / Firecracker micro‑VM, etc.).
"""
from __future__ import annotations

import traceback
import json
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO
from types import SimpleNamespace
from typing import Dict, Optional, Tuple, Callable

from johnllm import LMP, LLMModel  # type: ignore – project‑specific
from pydantic import BaseModel
from logging import getLogger

from scripts.portswigger.data.server_side import PORT_SWIGGER_LABS 

from .logger import setup_agent_logger

logger = getLogger("agentlog")

from typing import Generic, TypeVar

T = TypeVar("T")

class AgentOutput(BaseModel):
    """LLM response schema.

    The model must return JSON with these two keys. `reflection` is the
    natural‑language reasoning; `script` is the Python code to execute next.
    """
    reflection: str
    script: str
    answer: Optional[Dict] = None

    def is_complete(self):
        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection
    
class PentestBot(LMP):
    """Language‑model program that drives each reasoning step."""

    prompt = (
        """
{% if problem_stmt %}
<problem_stmt>
Here is the CTF problem statement:
{{ problem_stmt }}
</problem_stmt>
{% endif %}

{% if prev_turn %}
<prev_reflection>
Here is the reflection from the previous action
{{ prev_turn.reflection }}
</prev_reflection>

<prev_script>
Here is the generated script:
{{ prev_turn.script }}
</prev_script>

<prev_output>
Here is the output from the previous action:
{{ prev_turn.output }}
</prev_output>
{% endif %}

You are a professional web-app CTF/pentesting agent named **PentestBot**.
Your goal is to discover and exploit a vulnerability in the target.
The *only* interaction channel with the target is a Python script that will be
executed immediately. You will receive the combined stdout/stderr of that
execution to inform your next step.

<recommended_workflow>
Recommended workflow per turn:
1. Analyse the previous execution script and output.
2. Decide the next tactical objective.
3. Emit a concise reflection explaining your reasoning.
4. Emit a *complete* Python script implementing that objective.
</recommended_workflow>

* IMPORTANT: *
When you need to print the output from a http request, follow this guideline:
1. Always print (status_code, len_of_response_body)
2. Determine if 1. is enough to accomplish your current tactical objective. Make this decision explicit
in your reflections
3. Only print the full response if your outcome from 2. requires it for the determination of your tactical objective

If you find the answer according to criteria defined in problem_stmt, then return the answer
Otherwise return null for answer
"""
    )
    response_format = AgentOutput


###############################################################################
# Sandbox interpreter
###############################################################################


class PythonInterpreter:
    """Very small wrapper that runs untrusted Python code and captures output.

    For demonstration purposes `exec` is used. In real life, replace this with
    a hardened sandbox (Docker, gVisor, Firecracker, etc.) and apply resource
    limits.
    """

    def __init__(self, shared_globals: Optional[Dict] = None) -> None:
        # Allow stateful payloads (e.g. re‑using imported `requests` sessions)
        self._globals: Dict = shared_globals or {}

    # ---------------------------------------------------------------------
    # Public helpers
    # ---------------------------------------------------------------------

    def run(self, code: str) -> str:
        """Execute *code* and return the concatenated stdout+stderr text."""

        stdout_buf = StringIO()
        stderr_buf = StringIO()

        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except Exception:  # pylint: disable=broad-except
            traceback.print_exc(file=stderr_buf)

        stdout_text = stdout_buf.getvalue()
        stderr_text = stderr_buf.getvalue()
        return stdout_text + ("\n" + stderr_text if stderr_text else "")


###############################################################################
# Multi‑turn session driver
###############################################################################


class PentestSession:
    """Drives the LLM ↔ interpreter feedback loop."""

    def __init__(
        self,
        problem_stmt: str,
        model: LLMModel,
        model_name: str = "gpt-4.1",
        max_steps: int = 12,
        success_condition: str = "",
        eval_fn: Optional[Callable] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.model_name = model_name
        self.max_steps = max_steps
        self.success_condition = success_condition
        self.logger = getLogger("agentlog")
        self.reqs_logger = getLogger("full_requests")

        self._is_success = eval_fn or (lambda x: False)
        self._model = model
        self._agent = PentestBot()
        self._interp = PythonInterpreter()
        self._success = False
        self._steps = 0

    def _log(self, msg: str) -> None:
        self.logger.info(msg)
        self.reqs_logger.info(msg)

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:
        """Run the conversation loop until success or exhaustion."""
        prev_turn: Optional[SimpleNamespace] = None

        for step in range(1, self.max_steps + 1):
            self._log(f"========== STEP {step} ==========")

            # Build prompt args for this turn
            prompt_args = {
                "problem_stmt": self.problem_stmt,
                "prev_turn": prev_turn,
                # TODO: remove this ??
                "success_condition": self.success_condition,
            }
            self._log(f"Prompt: \n{self._agent._prepare_prompt(templates={}, **prompt_args)}")

            # Call the LLM
            agent_out: AgentOutput = self._agent.invoke(
                model=self._model,
                model_name=self.model_name,
                prompt_args=prompt_args,
            )
            if agent_out.answer:
                if self._is_success(agent_out.answer):
                    self._log("Agent successfully completed task with answer.")
                    return
                else:
                    raise ValueError(f"Wrong answer! : {agent_out.answer}")
                
            # Log reflection & script
            self._log(f"Reflection:\n{agent_out.reflection.strip()}")
            self._log(f"Generated script:\n{agent_out.script.rstrip()}")
            # Execute the script
            execution_result = self._interp.run(agent_out.script)
            self.reqs_logger.info("Truncated execution output (truncated to 8k chars):\n%s", execution_result)
            self.logger.info("Truncated execution output (truncated to 8k chars):\n%s", execution_result[:3])

            # Prepare previous‑turn context for next iteration
            prev_turn = SimpleNamespace(
                reflection=agent_out.reflection,
                script=agent_out.script,
                output=execution_result,
            )
            self._steps += 1

        self.logger.warning("💀 Maximum steps (%d) reached without confirmed exploit.", self.max_steps)

    def result(self):
        return self._success, self._steps, self.max_steps

def start_agent(lab_info, eval_name, model, max_steps, use_answer) -> tuple[bool, int, int]:  # pragma: no cover (simple demo runner)
    try:
        url, vuln, lab_ind = lab_info
        labs = PORT_SWIGGER_LABS[vuln]
        answer = labs[lab_ind].get("answer", "")
        problem_statement = labs[lab_ind]["prompt"].format(url=url, answer=answer if use_answer else "")
        success_condition = labs[lab_ind].get("success_condition", None)
        eval_fn = labs[lab_ind].get("eval_fn", None)

        setup_agent_logger(eval_name)

        session = PentestSession(   
            problem_stmt=problem_statement,
            model=model,
            model_name="command-r",
            # TODO: this success condition should go into PORTSWIGGER_JSON
            success_condition=success_condition, 
            max_steps=max_steps,
            eval_fn=eval_fn
        )
        session.run()
        return session.result()
    except Exception:                                  # noqa: BLE001, E722
        logger.exception("Uncaught error in %s", eval_name)
        # Return a sentinel failure that the caller can record
        return False, 0, max_steps

if __name__ == "__main__":    
    from concurrent.futures import ThreadPoolExecutor
    from dataclasses import dataclass
    from typing import Dict

    @dataclass 
    class AgentResult:
        eval_name: str
        success: bool
        steps: int
        max_steps: int

    USE_ANSWER = True
    MAX_STEPS = 12
    LAB_URLS = [
        ("https://0a680074038359fc80336cd60061005d.web-security-academy.net/", "server_side_request_forgery", 1),
        # ("https://0afe009304c3273880d1083e00d1001a.web-security-academy.net/", "sql_injection", 5),
        # ("https://0a3f00d703e0b0f8806d804800d60027.web-security-academy.net/", "file_upload", 4),
        # ("https://0ac300d203ed8b8a80c51cc1004c007d.web-security-academy.net/", "path_traversal", 2),
        # ("https://0a410002034cb97d800c083f00cd00bf.web-security-academy.net/", "sql_injection", 2),
        # ("https://0aeb000104254f5680055dd9007b0046.web-security-academy.net/", "sql_injection", 8),
    ]

    results: Dict[str, AgentResult] = {}
    model = LLMModel()
    
    def run_agent(lab_info: Tuple[str, str, int], i: int, use_answer: bool) -> None:
        eval_name = f"{lab_info[1]}_{lab_info[2]}"
        success, steps, max_steps = start_agent(lab_info, eval_name, model, MAX_STEPS, use_answer)
        results[eval_name] = AgentResult(eval_name, success, steps, max_steps)

    # Run agents in parallel
    with ThreadPoolExecutor() as executor:
        executor.map(run_agent, LAB_URLS, range(len(LAB_URLS)), [USE_ANSWER] * len(LAB_URLS))

    # Print results
    print("\nAgent Results:")   
    print("-" * 50)
    for eval_name, result in results.items():
        status = "✅ Success" if result.success else "❌ Failed"
        print(f"{eval_name}: {status} (Steps: {result.steps}/{result.max_steps})")

    print("MODEL COST: ", model.cost)